# -*- coding: utf-8 -*-
"""NER_multilingual_hugging_face

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LdeTesFrnV6jT0LTrY-rDB9-1UcpSQxG
"""

# build a multilingual transformer which enables zero shot cross-lingual transfer
# suitable for 'code-switching' where a speaker may alternate between two or more languages
# fine tune a XLM-RoBERTa model to perform NER across several languages
# the dataset is a subset of Cross-lingual Transfer Evalaution of Multilingual Encoders (XTREME) benchmark
# called WikiANN or PAN-X. It contains Wikipedia pages of many languages including four of the most
# common spoken in Switerland (German 62.9%, FRench 22.9%, Italian 8.4% and English 5.9%)
# each artical is annotated with LOC (location), PER(person), and ORG (organisation) tags in the IOB2 format
!pip install datasets

# load one of the PAN-X subsets
from datasets import get_dataset_config_names
xtreme_subsets = get_dataset_config_names("xtreme")
print(f"XTREME has {len(xtreme_subsets)} configurations")

# each PAN-X subset has a two-letter suffix which is the ISO 639-l language code
panx_subsets = [s for s in xtreme_subsets if s.startswith("PAN")]
panx_subsets[:3]

from datasets import load_dataset
load_dataset("xtreme", name="PAN-X.de")

# for a realistic Swiss corpus the German (de), French (fr), Italian (it) and English (en) will be sampled.
# the language dataset will be imbalanced but will be addressed later

from collections import defaultdict
from datasets import DatasetDict

langs = ["de", "fr", "it", "en"]
fracs = [0.629, 0.229, 0.084, 0.059]

# return a DatasetDict if a key doen't match
panx_ch = defaultdict(DatasetDict)

for lang, frac in zip(langs, fracs):
  # load monolingual corpus
  ds = load_dataset("xtreme", name=f"PAN-X.{lang}")
  # shuffle and downsample each split according to spoken proportion
  for split in ds:
    panx_ch[lang][split] = (
        ds[split]
        .shuffle(seed=0)
        .select(range(int(frac * ds[split].num_rows))))

import pandas as pd
pd.DataFrame({lang: [panx_ch[lang]["train"].num_rows] for lang in langs},
             index=["Number of training examples"])

# inspect one of the German examples in corpus.
# There are way more German examples, than all the other languages combined
# however it can be used to perform zero-shot cross-lingual transfer to
# French, Italian and English

element = panx_ch["de"]["train"][0]
for key, value in element.items():
  print(f"{key}: {value}")

# the ner_tags are mapped to numerically
# create a sperate column with familiar NER tags for human readability

for key, value in panx_ch["de"]["train"].features.items():
  print(f"{key}: {value}")

tags = panx_ch["de"]["train"].features["ner_tags"].feature
print(tags)

# create a new column in the training dataset using the int2str() method with class names for each tag
# the map () method returns a dict with the key corresponding to the new column name and the value as a list of class names
def create_tag_names(batch):
  return {"ner_tags_str": [tags.int2str(idx) for idx in batch["ner_tags"]]}

panx_de = panx_ch["de"].map(create_tag_names)

de_example = panx_de["train"][0]
pd.DataFrame([de_example["tokens"], de_example["ner_tags_str"]],
             ["Tokens", "Tags"])

# calculate the frequencies of each entity across each split
# to check for major imbalances
from collections import Counter
split2freqs = defaultdict(Counter)
for split, dataset in panx_de.items():
  for row in dataset["ner_tags_str"]:
    for tag in row:
      if tag.startswith("B"):
        tag_type = tag.split("-")[1]
        split2freqs[split][tag_type] += 1
pd.DataFrame.from_dict(split2freqs, orient="index")

# tokenizer for XLM-RoBERTa model, uses SentencePiece tokenizer and is trained on the raw text of 100 languages
# vocab size is 250,000 tokens and tokenises the raw text directly

from transformers import AutoTokenizer

bert_model_name = "bert-base-cased"
xlmr_model_name = "xlm-roberta-base"
bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)

# see the difference between the tokenizers
text = "Jack Sparrow loves New York!"
bert_tokens = bert_tokenizer(text).tokens()
xlmr_tokens = xlmr_tokenizer(text).tokens()
print(bert_tokenizer(text).tokens())
print(xlmr_tokenizer(text).tokens())

"".join(xlmr_tokens).replace(u"\u2581", " ")

# create custom model for token classification
# can import XLMRobertaForTokenClassification class from hugging face for this task also
# config_class uses the standard XLM-RoBERTA defaults parameters
# super() method initialises the config setup and loads the pretrained weights
# Roberta model used as the base model, and extended with a custom classification head
# consisting of a dropout and standard feed-forwrd layer


import torch.nn as nn
from transformers import XLMRobertaConfig
from transformers.modeling_outputs import TokenClassifierOutput
from transformers.models.roberta.modeling_roberta import RobertaModel
from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel
'''
class XLMRobertaForTokenClassification(RobertaPreTrainedModel):
  config_class = XLMRobertaConfig

  def __init__(self, config):
    #super().__init(config)
    self.num_labels = config.num_labels
    # load model body
    self.roberta = RobertaModel(config, add_pooling_layer=False)
    # set up classification head
    self.dropout = nn.Dropout(config.hidden_dropout_prob)
    self.classifier = nn.Linear(config.hidden_size, config.num_labels)
    # load and initialize weights
    self.init_weights()

  def forward(self, input_ids=None, attention_mask=None,
              token_type_ids=None, labels=None, **kwargs):
    # use model body to get encoder representations
    outputs = self.roberta(input_ids, attention_mask=attention_mask,
                           token_type_ids=token_type_ids, **kwargs)
    # apply classifier to encoder representations
    sequence_output = self.dropout(outputs[0])
    logits = self.classifier(sequence_output)
    # calculate losses
    loss = None
    if labels is not None:
      loss_fct = nn.CrossEntropyLoss()
      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
    # return model output object
    return TokenClassifierOutput(loss=loss, logits=logits,
                                 hidden_states=outputs.hidden_states,
                                 attentions=outputs.attentions)
 '''

import torch.nn as nn
from transformers import XLMRobertaConfig
from transformers.modeling_outputs import TokenClassifierOutput
from transformers.models.roberta.modeling_roberta import RobertaModel
from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel
from transformers import XLMRobertaForTokenClassification

# load the custom model
# provide tags to the model to label each entity and the mapping of each tag to an ID and vice versa

index2tag = {idx: tag for idx, tag in enumerate(tags.names)}
tag2index = {tag: idx for idx, tag in enumerate(tags.names)}

# pass these mappings to AutoConfig
from transformers import AutoConfig

xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,
                                         num_labels=tags.num_classes,
                                         id2label=index2tag, label2id=tag2index)

import torch
# load pretrained model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
xlmr_model = (XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device))

# check the model and tokenizer have initialised correctly
input_ids = xlmr_tokenizer.encode(text, return_tensors="pt")
pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=["Tokens", "Input IDs"])

# pass the inputs to the model and extract the predictions by taking the argmax
# to get the most likely class per token
# logits are [batch_size, num_tokens, num_tags]

outputs = xlmr_model(input_ids.to(device)).logits
predictions = torch.argmax(outputs, dim=-1),
print(f"Number of tokens in sequence: {len(xlmr_tokens)}")
print(f"Shape of outputs: {outputs.shape}")

# enumerate over the sequence to see what the the pretrained model predicts

def tag_text(text, tags, model, tokenizer):
  # get tokens with special characters
  tokens = tokenizer(text).tokens()
  # encode the sequence into IDs
  input_ids = xlmr_tokenizer(text, return_tensors="pt").input_ids.to(device)
  # get predictions as distribution over 7 possible classes
  outputs = model(input_ids)[0]
  # take argmax to get most likely class per token
  predictions = torch.argmax(outputs, dim=2)
  # convert to DataFrame
  preds = [tags.names[p] for p in predictions[0].cpu().numpy()]
  return pd.DataFrame([tokens, preds], index = ["Tokens", "Tags"])

# now the tokenizer and mode can encode a singel example
# next tokenise the entire dataset and pass to the XLM-RoBERTa model for fine-tuning

words, labels = de_example["tokens"], de_example["ner_tags"]
tokenized_input = xlmr_tokenizer(de_example["tokens"], is_split_into_words=True)
tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
pd.DataFrame([tokens], index=["Tokens"])

# To follow B-LOC convention and mask subwords using words_ids() from tokenized_input
# map each subword to the corresponding index in the words sequence

word_ids = tokenized_input.word_ids()
pd.DataFrame([tokens, word_ids], index=["Tokens", "Word IDs"])

# set special tokens [e.g., <s> and <\s> etc] to ID -100. In PyTorch the Cross-entropy loss class
# has an attribute ignore_index when set to -100, and therefore ignored during training

previous_word_idx = None
label_ids = []

for word_idx in word_ids:
  if word_idx is None or word_idx == previous_word_idx:
    label_ids.append(-100)
  elif word_idx != previous_word_idx:
    label_ids.append(labels[word_idx])
  previous_word_idx = word_idx

labels = [index2tag[l] if l != -100 else "IGN" for l in label_ids]
index = ["Tokens", "Word IDs", "Label IDs", "Labels"]

pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)

# scale above to handle the entire dataset

def tokenize_and_align_labels(examples):
  tokenized_inputs = xlmr_tokenizer(examples["tokens"], truncation=True,
                                    is_split_into_words=True)

  labels = []
  for idx, label in enumerate(examples["ner_tags"]):
    word_ids = tokenized_inputs.word_ids(batch_index=idx)
    previous_word_idx = None
    label_ids = []
    for word_idx in word_ids:
      if word_idx is None or word_idx == previous_word_idx:
        label_ids.append(-100)
      else:
        label_ids.append(label[word_idx])
      previous_word_idx = word_idx
    labels.append(label_ids)
  tokenized_inputs["labels"] = labels
  return tokenized_inputs

# function to iterate over each split

def encode_panx_dataset(corpus):
  return corpus.map(tokenize_and_align_labels, batched=True,
                    remove_columns=['langs', 'ner_tags', 'tokens'])

panx_de_encoded = encode_panx_dataset(panx_ch["de"])

!pip install seqeval

# performance metrics
# similiar to evaluating a text classification model
# commonly use precision, recall and f1
# however all words of an entity must be predicted correctly for the prediction to be counted as correct

from seqeval.metrics import classification_report

import numpy as np

def align_predictions(predictions, label_ids):
  preds = np.argmax(predictions, axis=2)
  batch_size, seq_len = preds.shape
  labels_list, preds_list = [], []

  for batch_idx in range(batch_size):
    example_labels, example_preds = [], []
    for seq_idx in range(seq_len):
      #ignore label IDs = -100
      if label_ids[batch_idx, seq_idx] != -100:
        example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])
        example_preds.append(index2tag[preds[batch_idx][seq_idx]])

    labels_list.append(example_labels)
    preds_list.append(example_preds)

  return preds_list, labels_list

!pip install transformers[torch]
#!pip install accelerate -U

# fine-tuning XLM-Roberta
# firstly, fine-tune the base model on the German subset of PAN-X
# secondly, evaluate its zero cross-lingual performance on French, Italian and English
# will need to restart the session for this cell to work

from transformers import TrainingArguments

num_epochs = 3
batch_size = 24
logging_steps = len(panx_de_encoded["train"]) // batch_size
model_name = f"{xlmr_model_name}-finetuned-panx-de"
training_args = TrainingArguments(
    output_dir=model_name, log_level='error', num_train_epochs=num_epochs,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size, evaluation_strategy="epoch",
    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,
    logging_steps=logging_steps, push_to_hub=True)

# connect with hugginface hub and push trained to model to hub
from huggingface_hub import notebook_login
notebook_login()

# compute predictions
from seqeval.metrics import f1_score
def compute_metrics(eval_pred):
  y_pred, y_true = align_predictions(eval_pred.predictions,
                                     eval_pred.label_ids)
  return {"f1": f1_score(y_true, y_pred)}

# pads the data sequences to the largest in length in the dataset
# padded with -100
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)

# this fx avoids initializing a new model for every Trainer by creating a model_init() method
# this method loads an untrained model
def model_init():
  return (XLMRobertaForTokenClassification
          .from_pretrained(xlmr_model_name, config=xlmr_config)
          .to(device))

from transformers import Trainer

trainer = Trainer(model_init=model_init, args=training_args,
                  data_collator=data_collator, compute_metrics=compute_metrics,
                  train_dataset=panx_de_encoded["train"],
                  eval_dataset=panx_de_encoded["validation"],
                  tokenizer=xlmr_tokenizer)

trainer.train()
trainer.push_to_hub(commit_message="Training completed!")

# test some text on the model
text_de = "Dave Power ist ein Infomatiker bei ASSERT in Cork, Irland."
tag_text(text_de, tags, trainer.model, xlmr_tokenizer)

# error analysis
# check to see if accidentally masked too many tokens or masked too many labels, which would artificially drop the loss
# comput_metrics() fx may have a bug that overestimates performance
# might include the zero class or 0 entity in NER as a normal class, which will heavily skew accuracy and F1
# if the model performs worse than expected, checking errors can identify bugs not noticed looking at the code
# even if no bugs and good performance, it is good practice to understance the models strengths and weaknesses
# good practice for deploying to prod

# check the examples with the highest validation loss

from torch.nn.functional import cross_entropy

def forward_pass_with_label(batch):
  # convert dict of lists to list of dicts suitable for data collator
  features = [dict(zip(batch, t)) for t in zip(*batch.values())]
  # pad inputs and labels and put all tensors on device
  batch = data_collator(features)
  input_ids = batch["input_ids"].to(device)
  attention_mask = batch["attention_mask"].to(device)
  labels = batch["labels"].to(device)
  with torch.no_grad():
    # pass data through model
    output = trainer.model(input_ids, attention_mask)
    # logit.size: [batch_size, sequence_length, classes]
    # predict class with largest logit value on classes axis
    predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()
  # calculate loss per token after flattening batch dimension with view
  loss = cross_entropy(output.logits.view(-1, 7),
                       labels.view(-1), reduction='none')
  # unflatten batch dimension and convert to numpy array
  loss = loss.view(len(input_ids), -1).cpu().numpy()

  return {"loss":loss, "predicted_label": predicted_label}

# apply forward_pass_with_label fx to whole validation set using map()
# and load to DataFrame for further analysis

valid_set = panx_de_encoded["validation"]
valid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)
df = valid_set.to_pandas()

index2tag[-100] = "IGN"
df["input_tokens"] = df["input_ids"].apply(
    lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))
df["predicted_label"] = df["predicted_label"].apply(
    lambda x: [index2tag[i] for i in x])
df["labels"] = df["labels"].apply(
    lambda x: [index2tag[i] for i in x])
df["loss"] = df.apply(
    lambda x: x["loss"][:len(x["input_ids"])], axis=1)
df["predicted_label"] = df.apply(
    lambda x: x["predicted_label"][:len(x["input_ids"])], axis=1)
df.head(1)

df_tokens = df.apply(pd.Series.explode)
df_tokens = df_tokens.query("labels != 'IGN'")
df_tokens["loss"] = df_tokens["loss"].astype(float).round(2)
df_tokens.head(7)

# group the data, and aggregate the losses for each token with count, mean and sum

(
    df_tokens.groupby("input_tokens")[["loss"]]
    .agg(["count", "mean", "sum"])
    .droplevel(level=0, axis=1) # get rid of multi-level columns
    .sort_values(by="sum", ascending=False)
    .reset_index()
    .round(2)
    .head(10)
    .T
)

# the whitespace token has the highest total sum, because it is the most common.
# however, the mean loss is the lowest meaning the model finds it easy to classify.
# words like 'in', 'von', 'der' and 'und' occur frequently and sometimes together
# with named entities so it's easy for the model to mix them up

# look at the losses for each class

(
    df_tokens.groupby("labels")[["loss"]]
    .agg(["count", "mean", "sum"])
    .droplevel(level=0, axis=1)
    .reset_index()
    .round(2)
    .T
)

# investiagte further with a confusion matrix
# the I-ORG and B-ORG are sometimes getting misclassified

from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
import matplotlib.pyplot as plt


def plot_confusion_matrix(y_preds, y_true, labels):
  cm = confusion_matrix(y_true, y_preds, normalize='true')
  fig, ax = plt.subplots(figsize=(6, 6))
  disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=labels)
  disp.plot(cmap="Blues", values_format=".2f", ax=ax, colorbar=False)
  plt.title("Normalized confusion matrix")
  plt.show()

plot_confusion_matrix(df_tokens["labels"], df_tokens["predicted_label"],
                      tags.names)

# now that the errors are checked at token level
# check the errors at sequence level

def get_samples(df):
  for _, row in df.iterrows():
    labels, preds, tokens, losses = [], [], [], []
    for i, mask in enumerate(row["attention_mask"]):
      if i not in {0, len(row["attention_mask"])}:
        labels.append(row["labels"][i])
        preds.append(row["predicted_label"][i])
        tokens.append(row["input_tokens"][i])
        losses.append(f"{row['loss'][i]:.2f}")
    df_tmp = pd.DataFrame({"tokens": tokens, "labels": labels,
                           "preds": preds, "losses": losses}).T
    yield df_tmp

df["total_loss"] = df["loss"].apply(sum)
df_tmp = df.sort_values(by="total_loss", ascending=False).head(3)

for sample in get_samples(df_tmp):
  display(sample)

# something wrong with the labels as the united nations and central africian republic are labeled as PER and juli as an ORG
# the annotations in PAN_X were automated, known as 'silver standard'

# it seems the automated annotation tagged paraenthesis also as ORG
# this error analysis had identified weaknesses and potential inplications for downstream performance

df_tmp = df.loc[df["input_tokens"].apply(lambda x: u"\u2581(" in x)].head(2)
for sample in get_samples(df_tmp):
  display(sample)

# cross-lingual transfer

# evaluate the ability to transfer to other languages using the predict() method of Trainer

def get_f1_score(trainer, dataset):
  return trainer.predict(dataset).metrics["test_f1"]

# determine performance

f1_scores = defaultdict(dict)
f1_scores["de"]["de"] = get_f1_score(trainer, panx_de_encoded["test"])
print(f"F1-score of [de] model on [de] dataset: {f1_scores['de']['de']:.3f}")

# test French sample

text_fr = "Jeff Dean est informaticien chez Google en Californie"
tag_text(text_fr, tags, trainer.model, xlmr_tokenizer)

# test German model on the whole French test set
def evaluate_lang_performance(lang, trainer):
  panx_ds = encode_panx_dataset(panx_ch[lang])
  return get_f1_score(trainer, panx_ds['test'])

f1_scores['de']['fr'] = evaluate_lang_performance('fr', trainer)
print(f"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}")

# 0.7 is good considering it has not seen a French word yet
# German and French are not similiar

# test German model on the whole Italian test set
f1_scores['de']['it'] = evaluate_lang_performance('it', trainer)
print(f"F1-score of [de] model on [it] dataset: {f1_scores['de']['it']:.3f}")

# test German model on the whole English test set
f1_scores['de']['en'] = evaluate_lang_performance('en', trainer)
print(f"F1-score of [de] model on [en] dataset: {f1_scores['de']['en']:.3f}")

# it performs worst on English which is suprising

# fine-tune XLM-RoBERTa on the French corpus on training sets on increasing size.
# This way by tracking performance it can be determined at which point zero-point cross-lingual tranfer is superior

def train_on_subset(dataset, num_samples):
  train_ds = dataset["train"].shuffle(seed=42).select(range(num_samples))
  valid_ds = dataset["validation"]
  test_ds = dataset["test"]
  training_args.logging_steps = len(train_ds) // batch_size

  trainer = Trainer(model_init=model_init, args=training_args,
                    data_collator=data_collator, compute_metrics=compute_metrics,
                    train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)
  trainer.train()
  if training_args.push_to_hub:
    trainer.push_to_hub(commit_message='Training completed!')

    f1_score = get_f1_score(trainer, test_ds)
    return pd.DataFrame.from_dict(
        {"num_samples": [len(train_ds)], "f1_score": [f1_score]}
    )

# encode the French corpus into input IDs, attentions masks, and label IDs, like the German corpus
panx_fr_encoded = encode_panx_dataset(panx_ch["fr"])

# check fx works on sample of 250 examples [ gives very poor performance]
training_args.push_to_hub = False
metrics_df = train_on_subset(panx_fr_encoded, 250)

metrics_df

# increase the training set sizes and check performance

import pandas as pd

metrics_df = pd.DataFrame()

for num_samples in [500, 1000, 2000, 4000]:
    metrics_df = metrics_df.append(
        train_on_subset(panx_fr_encoded, num_samples), ignore_index=True
    )

fig, ax = plt.subplots()
ax.axhline(f1_scores["de"]["fr"], ls="--", color="r")
metrics_df.set_index("num_samples").plot(ax=ax)
plt.legend(["Zero-shot from de", "Fine-tunes on fr"], loc="lower right")
plt.ylim((0, 1))
plt.xlabel("Number of Training Samples")
plt.ylabel("F1 Score")
plt.show()

# concatenate datasets

from datasets import concatenate_datasets

def concatenate_splits(corpora):
  multi_corpus = DatasetDict()
  for split in corpora[0].keys():
    multi_corpus[split] = concatenate_datasets(
        [corpus[split] for corpus in corpora]).shuffle(seed=42)
  return multi_corpus

# conacatenate German and French datasets
panx_de_fr_encoded = concatenate_splits([panx_de_encoded, panx_fr_encoded])

# train model

training_args.logging_steps = len(panx_de_fr_encoded["train"]) // batch_size
training_args.push_to_hub = True
training_args.output_dir = "xlm-roberta-base-finetuned-panx-de-fr"

trainer = Trainer(model_init=model_init, args=training_args,
                  data_collator=data_collator, compute_metrics=compute_metrics,
                  tokenizer=xlmr_tokenizer, train_dataset=panx_de_fr_encoded["train"],
                  eval_dataset=panx_de_fr_encoded["validation"])
trainer.train()
trainer.push_to_hub(commit_message="Training completed!")

corpora= [panx_de_encoded]

# exclude German from iteration
for lang in langs[1:]:
  training_args.output_dir = f"xlm-roberta-base-finetuned-panx-{lang}"
  # fine-tune on monolingual corpus
  ds_encoded = encode_panx_dataset(panx_ch[lang])
  metrics = train_on_subset(ds_encoded, ds_encoded["train"].num_rows)
  # collect F1-scores in common dict
  f1_scores[lang][lang] = metrics["f1_score"][0]
  # add monolingual corpus to list of corpora to concatenate
  corpora.append(ds_encoded)

# following fine-tuning on each language corpus, concatenate all the splits into
# a multilingual corpus of all four languages
corpora_encoded = concatenate_splits(corpora)

# train the model
training_args.logging_steps = len(corpora_encoded["train"]) // batch_size
training_args.output_dir = "xlm-roberta-base-finetuned-panx-all"

trainer = Trainer(model_init=model_init, args=training_args,
                  data_collator=data_collator, compute_metrics=compute_metrics,
                  tokenizer=xlmr_tokenizer, train_dataset=corpora_encoded["train"],
                  eval_dataset=corpora_encoded["validation"])

trainer.train()
trainer.push_to_hub(commit_message="Training completed!")

# generate predictions from the trainer on each language test set

for idx, lang in enumerate(langs):
  f1_scores["all"][lang] = get_f1_score(trainer, corpora[idx]["test"])

  scores_data = {"de": f1_scores["de"],
                 "each": {lang: f1_scores[lang][lang] for lang in langs},
                 "all": f1_scores["all"]}
  f1_scores_df = pd.DataFrame(scores_data).T.round(4)
  f1_scores_df.rename_axis(index="Fine-tine on", columns="Evaluation on",
                           inplace=True)

f1_scores_df